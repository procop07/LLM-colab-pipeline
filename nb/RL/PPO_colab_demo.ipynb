{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# PPO (Proximal Policy Optimization) Training Demo\n",
        "\n",
        "This notebook demonstrates how to train language models using PPO for RLHF (Reinforcement Learning from Human Feedback).\n",
        "\n",
        "**Key Features:**\n",
        "- PPO algorithm implementation\n",
        "- Human feedback simulation\n",
        "- Reward model training\n",
        "- Policy optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install TRL and dependencies\n",
        "!pip install -q trl transformers torch accelerate peft\n",
        "!pip install -q datasets wandb\n",
        "\n",
        "import torch\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# PPO Configuration\n",
        "config = PPOConfig(\n",
        "    model_name=\"microsoft/DialoGPT-medium\",\n",
        "    learning_rate=1.41e-5,\n",
        "    batch_size=16,\n",
        "    mini_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optimize_cuda_cache=True,\n",
        "    early_stopping=False,\n",
        "    target_kl=0.1,\n",
        "    ppo_epochs=4,\n",
        "    seed=0,\n",
        "    init_kl_coef=0.2,\n",
        "    adap_kl_ctrl=True\n",
        ")\n",
        "\n",
        "print(\"PPO Configuration:\")\n",
        "print(f\"Model: {config.model_name}\")\n",
        "print(f\"Learning Rate: {config.learning_rate}\")\n",
        "print(f\"Batch Size: {config.batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded: {model.__class__.__name__}\")\n",
        "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reward_function"
      },
      "outputs": [],
      "source": [
        "# Simple reward function (replace with actual reward model)\n",
        "def compute_reward(texts):\n",
        "    \"\"\"\n",
        "    Simple reward function - in practice, you would use\n",
        "    a trained reward model or human feedback\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for text in texts:\n",
        "        # Simple heuristic: reward longer, more diverse responses\n",
        "        length_reward = min(len(text.split()) / 20.0, 1.0)\n",
        "        diversity_reward = len(set(text.lower().split())) / max(len(text.split()), 1)\n",
        "        reward = length_reward + diversity_reward\n",
        "        rewards.append([reward])\n",
        "    return rewards\n",
        "\n",
        "# Test reward function\n",
        "test_texts = [\"Hello world\", \"This is a longer and more diverse sentence with various words\"]\n",
        "test_rewards = compute_reward(test_texts)\n",
        "print(f\"Test rewards: {test_rewards}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_data"
      },
      "outputs": [],
      "source": [
        "# Create sample training data\n",
        "prompts = [\n",
        "    \"What is artificial intelligence?\",\n",
        "    \"Explain machine learning\",\n",
        "    \"How does deep learning work?\",\n",
        "    \"What are neural networks?\",\n",
        "    \"Describe natural language processing\"\n",
        "]\n",
        "\n",
        "# Tokenize prompts\n",
        "tokenized_prompts = []\n",
        "for prompt in prompts:\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    tokenized_prompts.append(tokens.squeeze())\n",
        "\n",
        "print(f\"Number of prompts: {len(prompts)}\")\n",
        "print(f\"Sample prompt: {prompts[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppo_training"
      },
      "outputs": [],
      "source": [
        "# Initialize PPO trainer\n",
        "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):  # Small number for demo\n",
        "    print(f\"\\nEpoch {epoch + 1}\")\n",
        "    \n",
        "    for batch_idx in range(0, len(tokenized_prompts), config.batch_size):\n",
        "        batch_prompts = tokenized_prompts[batch_idx:batch_idx + config.batch_size]\n",
        "        \n",
        "        # Generate responses\n",
        "        response_tensors = []\n",
        "        for prompt in batch_prompts:\n",
        "            response = ppo_trainer.generate(\n",
        "                prompt.unsqueeze(0),\n",
        "                max_length=prompt.shape[0] + 50,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            response_tensors.append(response.squeeze())\n",
        "        \n",
        "        # Decode responses\n",
        "        responses = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]\n",
        "        \n",
        "        # Compute rewards\n",
        "        rewards = compute_reward(responses)\n",
        "        rewards = [torch.tensor(r) for r in rewards]\n",
        "        \n",
        "        # PPO step\n",
        "        stats = ppo_trainer.step(batch_prompts, response_tensors, rewards)\n",
        "        \n",
        "        print(f\"Batch {batch_idx // config.batch_size + 1}: Mean reward = {np.mean([r.item() for r in rewards]):.3f}\")\n",
        "        \n",
        "print(\"\\nTraining completed!\")"
      ]
    }
  ]
}
