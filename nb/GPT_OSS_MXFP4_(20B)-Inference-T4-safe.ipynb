{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# GPT OSS MXFP4 (20B) Inference - T4 Safe Pipeline\n",
        "\n",
        "This notebook provides a T4-safe implementation for running GPT models with mixed-precision inference.\n",
        "\n",
        "**Features:**\n",
        "- 20B parameter model support\n",
        "- Mixed precision (FP4) optimization\n",
        "- T4 GPU compatible memory management\n",
        "- Safe inference with error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch transformers accelerate bitsandbytes\n",
        "!pip install -q peft trl datasets\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-config"
      },
      "outputs": [],
      "source": [
        "# T4-Safe Configuration for 20B models\n",
        "model_name = \"microsoft/DialoGPT-large\"  # Replace with your 20B model\n",
        "\n",
        "# BitsAndBytes configuration for FP4\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-model"
      },
      "outputs": [],
      "source": [
        "# Load model with T4-safe settings\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    print(\"Model loaded successfully!\")\n",
        "    print(f\"Model device: {next(model.parameters()).device}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Please check model name and GPU memory availability\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference-pipeline"
      },
      "outputs": [],
      "source": [
        "# Safe inference pipeline\n",
        "def safe_generate(prompt, max_length=512, temperature=0.7, top_p=0.9):\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"Error during generation: {e}\"\n",
        "\n",
        "# Test the pipeline\n",
        "test_prompt = \"The future of artificial intelligence is\"\n",
        "result = safe_generate(test_prompt)\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Generated: {result}\")"
      ]
    }
  ]
}
