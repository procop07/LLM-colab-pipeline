# LLM-colab-pipeline

A comprehensive pipeline for Large Language Model development with Google Colab notebooks, including inference optimization and reinforcement learning training methods.

## ğŸš€ Features

- **T4-Safe Inference**: Optimized for Google Colab T4 GPUs with memory-efficient mixed precision
- **Multiple RL Training Methods**: PPO, DPO, and ORPO implementations
- **Ready-to-Use Notebooks**: Pre-configured Colab notebooks for immediate deployment
- **Production-Ready Code**: Error handling, memory optimization, and best practices

## ğŸ“ Project Structure

```
LLM-colab-pipeline/
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ nb/                          # Notebook directory
â”‚   â”œâ”€â”€ GPT_OSS_MXFP4_(20B)-Inference-T4-safe.ipynb  # GPT inference notebook
â”‚   â””â”€â”€ RL/                      # Reinforcement Learning notebooks
â”‚       â”œâ”€â”€ PPO_colab_demo.ipynb    # Proximal Policy Optimization
â”‚       â”œâ”€â”€ DPO_colab_demo.ipynb    # Direct Preference Optimization
â”‚       â””â”€â”€ ORPO_colab_demo.ipynb   # Odds Ratio Preference Optimization
â””â”€â”€ README.md                    # This file
```

## ğŸ› ï¸ Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/procop07/LLM-colab-pipeline.git
   cd LLM-colab-pipeline
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“š Notebooks Guide

### ğŸ”¥ Inference Notebook

**[GPT_OSS_MXFP4_(20B)-Inference-T4-safe.ipynb](./nb/GPT_OSS_MXFP4_(20B)-Inference-T4-safe.ipynb)**

- **Purpose**: T4-safe inference for large language models (up to 20B parameters)
- **Features**: 
  - Mixed precision (FP4) quantization
  - Memory-efficient loading
  - Error handling and safe inference
- **Compatible with**: T4, P100, V100 GPUs

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/procop07/LLM-colab-pipeline/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference-T4-safe.ipynb)

### ğŸ¯ Reinforcement Learning Notebooks

#### **[PPO_colab_demo.ipynb](./nb/RL/PPO_colab_demo.ipynb)**

- **Algorithm**: Proximal Policy Optimization
- **Use Case**: RLHF (Reinforcement Learning from Human Feedback)
- **Features**: Reward model integration, policy optimization

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/procop07/LLM-colab-pipeline/blob/main/nb/RL/PPO_colab_demo.ipynb)

#### **[DPO_colab_demo.ipynb](./nb/RL/DPO_colab_demo.ipynb)**

- **Algorithm**: Direct Preference Optimization
- **Use Case**: Simpler alternative to RLHF without reward model
- **Features**: Preference-based training, stable optimization

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/procop07/LLM-colab-pipeline/blob/main/nb/RL/DPO_colab_demo.ipynb)

#### **[ORPO_colab_demo.ipynb](./nb/RL/ORPO_colab_demo.ipynb)**

- **Algorithm**: Odds Ratio Preference Optimization
- **Use Case**: Reference-model-free alignment
- **Features**: Memory efficient, single-stage training

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/procop07/LLM-colab-pipeline/blob/main/nb/RL/ORPO_colab_demo.ipynb)

## ğŸš¦ Quick Start

### For Inference:
1. Open the [GPT Inference notebook](./nb/GPT_OSS_MXFP4_(20B)-Inference-T4-safe.ipynb) in Colab
2. Run all cells to install dependencies
3. Replace the model name with your desired model
4. Execute the inference pipeline

### For RL Training:
1. Choose your preferred RL method (PPO, DPO, or ORPO)
2. Open the corresponding notebook in Colab
3. Prepare your training data in the specified format
4. Run the training pipeline

## ğŸ’¡ Usage Tips

- **GPU Requirements**: T4 or higher recommended for inference, P100+ for training
- **Memory Management**: Use gradient checkpointing for large models
- **Batch Sizes**: Start small (1-2) and increase based on available memory
- **Monitoring**: Use wandb integration for experiment tracking

## ğŸ”§ Customization

### Adding New Models
1. Update the model name in the notebook configuration
2. Adjust quantization settings if needed
3. Modify generation parameters as required

### Custom Training Data
1. Follow the data format shown in the notebooks
2. Ensure proper prompt-response structure
3. Balance positive and negative examples

## ğŸ“ˆ Performance

| Model Size | Method | T4 GPU | Training Time | Memory Usage |
|------------|--------|---------|---------------|-------------|
| 7B         | DPO    | âœ…      | ~2 hours      | ~14GB       |
| 13B        | PPO    | âš ï¸      | ~4 hours      | ~15GB       |
| 20B        | ORPO   | âŒ      | N/A           | >16GB       |

**Legend**: âœ… Recommended, âš ï¸ Possible with optimizations, âŒ Not recommended

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ”— Links

- **Repository**: [https://github.com/procop07/LLM-colab-pipeline](https://github.com/procop07/LLM-colab-pipeline)
- **Issues**: [Report bugs and request features](https://github.com/procop07/LLM-colab-pipeline/issues)
- **Discussions**: [Community discussions](https://github.com/procop07/LLM-colab-pipeline/discussions)

## â­ Star History

If you find this project helpful, please consider giving it a star! â­

---

**Made with â¤ï¸ for the LLM community**
